# KevinCY-Kodex — Testing Strategy
**Version:** 2025.11.27 (Universal Standard)
**Scope:** FastAPI · AI/RAG · Clean Architecture · Pytest (Async)

---

## 1. Testing Philosophy

The testing philosophy of `KevinCY-Kodex` is **"Continuously delivering reliable code"**. All code must prove its quality through tests.

-   **Quality Assurance**: Guarantee that new features work as intended.
-   **Regression Prevention**: Prevent existing features from being damaged by code changes.
-   **Confident Refactoring**: Tests act as a safety net, allowing code structure improvement with confidence.
-   **Living Documentation**: Well-written test code serves as a specification of the feature itself.

---

## 2. Test Types and Scope

The project manages quality centering on three types of tests.

1.  **Unit Test**
    -   **Scope**: Single function, method, or class.
    -   **Characteristics**: All external dependencies (DB, API, AI Model, etc.) are **Mocked**. Runs in a very fast and isolated environment.
    -   **Goal**: Verification of logic correctness.

2.  **Integration Test**
    -   **Scope**: Testing two or more layers working together. (e.g., `Service` + `Repository`)
    -   **Characteristics**: May include some integration with actual DB (for testing) or external systems. Slower than unit tests.
    -   **Goal**: Verification of interaction and data flow between layers.

3.  **E2E Test (End-to-End Test)**
    -   **Scope**: Testing the entire flow from API endpoint request to response, identical to actual user scenarios.
    -   **Characteristics**: Slowest, but most certainly guarantees the operation of the entire system.
    -   **Goal**: Verification of system stability and operation from the end-user perspective.

---

## 3. Layer-by-Layer Strategy

Each layer of Clean Architecture is tested according to the following strategies.

### 3.1 `/app/routers`
-   **Test Targets**:
    -   Does it return `200 OK` status code for normal requests?
    -   Does it return `422 Unprocessable Entity` for invalid requests (validation failure)?
    -   Is `RequestModel` correctly passed to the `Service` layer?
    -   Is the return value of `Service` returned according to `ResponseModel` format?
-   **Tools**: FastAPI's `TestClient`

### 3.2 `/app/services` (Async Logic)
-   **Test Targets**:
    -   All branches of core business logic (if/else, etc.).
    -   **Hybrid Fallback**: Does logic switching to Local LLM work upon API call failure?
    -   Does it call functions of `Repository` or `AI` modules with correct arguments?
    -   Does it correctly handle exceptions?
-   **Tools**: `pytest`, `pytest-mock`, `pytest-asyncio` (For async function testing)

### 3.3 `/app/repositories`
-   **Test Targets**:
    -   Are DB queries correctly written and executed?
    -   Does it request with correct endpoints and parameters when calling external APIs?
    -   Does it convert DB or API responses into correct data objects?
-   **Tools**: `pytest`, `pytest-mock`, `httpx` (For external API mocking), Test DB Session

---

## 4. AI Pipeline Testing Strategy

Considering the non-deterministic nature of AI models, focus on testing the **structure and format** rather than the **content** of the results.
*(Quality evaluation follows Evaluation Rules in `RAG.md`.)*

### 4.1 RAG Retrieval Test
-   **Goal**: Verify if intended core documents (Chunks) are included in the top N search results for a specific question.
-   **Method**: Create predefined "Question-Mandatory Chunk ID" pairs, and check if mandatory chunks are included in the result after running `retriever` function using `assert` statements.

### 4.2 LLM Response Generation Test (Hybrid Strategy)
-   **Goal**: Verify the **Format** and **Fallback Logic** of answers generated by LLM.
-   **Method**:
    -   **Mocking**: Mock `SKT A.X` and `Ollama` clients respectively to return fixed dummy text.
    -   **Structure Check**: Verify if the answer generation function receives this dummy text and finally returns a valid JSON structure (`answer`, `sources`).

---

## 5. Test Environment & Tools

-   **Test Runner**: `pytest`
-   **Async Plugin**: `pytest-asyncio` (Mandatory for FastAPI/LangGraph async testing)
-   **Mocking Library**: `pytest-mock`
-   **HTTP Client (For API Testing)**: `TestClient` (from `fastapi.testclient`)
-   **HTTP Client (For External API Mocking)**: `httpx`

---

## 6. Test Execution Rules

1.  All new feature additions and bug fixes must include relevant test codes.
2.  All tests (`unit`, `integration`) must pass before merging code into the `main` branch.
3.  Test codes must also comply with all rules (Naming, Type Hint) in `Code_Style.md`.
4.  It is recommended to maintain target test coverage of **over 80%**.

---

## 7. Agent Protocol (Antigravity Guide)
*Rules for agents to follow when performing tests in the terminal.*

1.  **Command:** Use the following commands when running tests.
    -   All tests: `pytest`
    -   Include detailed logs: `pytest -v`
    -   Specific file only: `pytest tests/test_chat_service.py`
2.  **Safety First:**
    -   Before running tests, check if `.env` file points to **Test Environment** settings (e.g., `TEST_DB_URL`).
    -   **Never run tests while connected to Production DB.**

End of KevinCY-Kodex Testing Strategy